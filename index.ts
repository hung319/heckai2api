/**
 * =================================================================================
 * Project: heck-2api (Bun Edition)
 * Version: 2.0.0 (Bun Native)
 * Author: Senior Software Engineer (Ported by CezDev)
 *
 * [T√≠nh nƒÉng]
 * 1. Bun Native Server (High Performance).
 * 2. H·ªó tr·ª£ chu·∫©n OpenAI (Stream & Non-Stream).
 * 3. T·ª± ƒë·ªông x·ª≠ l√Ω Session n·∫∑c danh.
 * 4. H·ªó tr·ª£ DeepSeek Reasoning (reasoning_content).
 * =================================================================================
 */

import { randomUUID } from "crypto";

// --- [C·∫•u h√¨nh] ---
const CONFIG = {
  PORT: parseInt(process.env.PORT || "3000"),
  API_KEY: process.env.API_MASTER_KEY || "1",
  UPSTREAM_API_BASE: process.env.UPSTREAM_API_BASE || "https://api.heckai.weight-wave.com/api/ha/v1",
  
  // Headers gi·∫£ l·∫≠p tr√¨nh duy·ªát
  HEADERS: {
    "Host": "api.heckai.weight-wave.com",
    "Origin": "https://heck.ai",
    "Referer": "https://heck.ai/",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36",
    "Content-Type": "application/json",
    "Accept": "*/*",
    "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
  },

  // Mapping model (OpenAI -> Heck)
  MODEL_MAP: {
    "gpt-4o-mini": "openai/gpt-4o-mini",
    "gpt-4o": "openai/chatgpt-4o-latest",
    "deepseek-r1": "deepseek/deepseek-r1",
    "deepseek-v3": "deepseek/deepseek-chat",
    "gemini-2.5-flash": "google/gemini-2.5-flash-preview",
    "claude-3.7-sonnet": "anthropic/claude-3.7-sonnet",
  } as Record<string, string>,

  DEFAULT_MODEL: "openai/gpt-4o-mini"
};

// --- [Helpers] ---

function corsHeaders() {
  return {
    "Access-Control-Allow-Origin": "*",
    "Access-Control-Allow-Methods": "GET, POST, OPTIONS",
    "Access-Control-Allow-Headers": "Content-Type, Authorization",
  };
}

function verifyAuth(req: Request): boolean {
  const authHeader = req.headers.get("Authorization");
  if (!authHeader) return CONFIG.API_KEY === "1"; // N·∫øu key l√† "1" th√¨ m·ªü c√¥ng khai (dev mode)
  const token = authHeader.replace("Bearer ", "").trim();
  return token === CONFIG.API_KEY;
}

// T·∫°o Session m·ªõi t·ª´ Upstream
async function createSession(title = "Chat") {
  try {
    const res = await fetch(`${CONFIG.UPSTREAM_API_BASE}/session/create`, {
      method: "POST",
      headers: CONFIG.HEADERS,
      body: JSON.stringify({ title }),
    });
    
    if (!res.ok) throw new Error(`Failed to create session: ${res.status}`);
    const data = await res.json() as any;
    return data.id;
  } catch (e) {
    console.error("Session Error:", e);
    throw e;
  }
}

// --- [Core Logic: Stream Parser - ƒê√£ s·ª≠a l·ªói d√≠nh ch·ªØ] ---

async function* streamProcessor(upstreamResponse: Response, requestId: string, model: string) {
  const reader = upstreamResponse.body?.getReader();
  if (!reader) throw new Error("No response body from upstream");

  const decoder = new TextDecoder();
  let buffer = "";
  let isReasoning = false;

  try {
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      buffer += decoder.decode(value, { stream: true });
      const lines = buffer.split("\n");
      buffer = lines.pop() || "";

      for (const line of lines) {
        // Ch·ªâ x·ª≠ l√Ω d√≤ng b·∫Øt ƒë·∫ßu b·∫±ng "data: "
        if (!line.startsWith("data: ")) continue;
        
        // L·∫•y n·ªôi dung th√¥, ch·ªâ c·∫Øt b·ªè "data: " (6 k√Ω t·ª±)
        // QUAN TR·ªåNG: Kh√¥ng d√πng .trim() ·ªü ƒë√¢y v√¨ s·∫Ω m·∫•t d·∫•u c√°ch ƒë·∫ßu t·ª´
        let dataStr = line.slice(6);

        // Lo·∫°i b·ªè k√Ω t·ª± \r n·∫øu c√≥ (do split \n ƒë·ªÉ l·∫°i)
        if (dataStr.endsWith("\r")) {
            dataStr = dataStr.slice(0, -1);
        }

        // Ki·ªÉm tra c√°c th·∫ª ƒëi·ªÅu khi·ªÉn (C·∫ßn trim t·∫°m ƒë·ªÉ so s√°nh ch√≠nh x√°c)
        const tagCheck = dataStr.trim();
        
        if (["[ANSWER_DONE]", "[RELATE_Q_START]", "[RELATE_Q_DONE]", "[ANSWER_START]"].includes(tagCheck)) continue;
        if (tagCheck === "[REASON_START]") { isReasoning = true; continue; }
        if (tagCheck === "[REASON_DONE]") { isReasoning = false; continue; }
        if (tagCheck === "[ERROR]") continue;

        // X·ª≠ l√Ω d·∫•u sao (g·ª£i √Ω c√¢u h·ªèi) n·∫øu c√≥: Thay v√¨ d√≠nh ch√πm, ta xu·ªëng d√≤ng
        if (dataStr.includes("‚ú©")) {
            dataStr = dataStr.replace(/‚ú©/g, "\n\nüí° G·ª£i √Ω: ");
        }

        // X·ª≠ l√Ω n·ªôi dung
        let chunk: any = null;
        
        if (isReasoning) {
          chunk = {
            id: requestId,
            object: "chat.completion.chunk",
            created: Math.floor(Date.now() / 1000),
            model: model,
            choices: [{ index: 0, delta: { reasoning_content: dataStr }, finish_reason: null }]
          };
        } else {
          chunk = {
            id: requestId,
            object: "chat.completion.chunk",
            created: Math.floor(Date.now() / 1000),
            model: model,
            choices: [{ index: 0, delta: { content: dataStr }, finish_reason: null }]
          };
        }

        yield `data: ${JSON.stringify(chunk)}\n\n`;
      }
    }
    yield `data: [DONE]\n\n`;
  } catch (e) {
    console.error("Stream processing error:", e);
    const errChunk = {
      id: requestId,
      object: "chat.completion.chunk",
      created: Math.floor(Date.now() / 1000),
      model: model,
      choices: [{ index: 0, delta: { content: `\n[Error: Upstream stream failed]` }, finish_reason: "stop" }]
    };
    yield `data: ${JSON.stringify(errChunk)}\n\n`;
  } finally {
    reader.releaseLock();
  }
}

// --- [Handlers] ---

async function handleChatCompletions(req: Request): Promise<Response> {
  let body: any;
  try {
    body = await req.json();
  } catch {
    return Response.json({ error: "Invalid JSON" }, { status: 400 });
  }

  const requestId = `chatcmpl-${randomUUID()}`;
  const requestModel = body.model || "gpt-4o-mini";
  // Logic Map Model
  let upstreamModel = CONFIG.MODEL_MAP[requestModel] || requestModel;
  if (!Object.values(CONFIG.MODEL_MAP).includes(upstreamModel) && !CONFIG.MODEL_MAP[requestModel]) {
     upstreamModel = CONFIG.DEFAULT_MODEL;
  }

  // X·ª≠ l√Ω Messages -> Prompt (Heck d√πng prompt text)
  let fullPrompt = "";
  let lastUserMsg = "";
  for (const msg of (body.messages || [])) {
    if (msg.role === "system") fullPrompt += `[System]: ${msg.content}\n`;
    else if (msg.role === "user") {
      fullPrompt += `[User]: ${msg.content}\n`;
      lastUserMsg = msg.content;
    }
    else if (msg.role === "assistant") fullPrompt += `[Assistant]: ${msg.content}\n`;
  }
  const question = fullPrompt.trim() || "Hello";

  // 1. T·∫°o Session ID (Anonymous)
  const sessionTitle = lastUserMsg.substring(0, 10) || "Chat";
  let sessionId;
  try {
    sessionId = await createSession(sessionTitle);
  } catch (e) {
    return Response.json({ error: { message: "Upstream session creation failed", type: "upstream_error" } }, { status: 502 });
  }

  // 2. G·ªçi Upstream
  const upstreamPayload = {
    model: upstreamModel,
    question: question,
    language: "Chinese", // C√≥ th·ªÉ ch·ªânh th√†nh "English" ho·∫∑c "Vietnamese" t√πy nhu c·∫ßu
    sessionId: sessionId,
    previousQuestion: null,
    previousAnswer: null,
    imgUrls: [],
    superSmartMode: false // B·∫≠t true n·∫øu mu·ªën √©p ch·∫ø ƒë·ªô suy nghƒ© s√¢u
  };

  const upstreamRes = await fetch(`${CONFIG.UPSTREAM_API_BASE}/chat`, {
    method: "POST",
    headers: CONFIG.HEADERS,
    body: JSON.stringify(upstreamPayload)
  });

  if (!upstreamRes.ok) {
    return Response.json({ error: { message: `Upstream error: ${upstreamRes.status}` } }, { status: upstreamRes.status });
  }

  // 3. X·ª≠ l√Ω ph·∫£n h·ªìi (Stream vs Non-Stream)
  const isStream = body.stream === true;

  if (isStream) {
    // --- Streaming Mode ---
    const stream = streamProcessor(upstreamRes, requestId, requestModel);
    // @ts-ignore: Bun supports async generator as body
    return new Response(stream, {
      headers: {
        ...corsHeaders(),
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
        "X-Heck-Session-Id": sessionId
      }
    });
  } else {
    // --- Non-Streaming Mode (Buffer to√†n b·ªô) ---
    // Ch√∫ng ta ph·∫£i ti√™u th·ª• streamProcessor ƒë·ªÉ l·∫•y to√†n b·ªô text
    let fullContent = "";
    let fullReasoning = "";
    const stream = streamProcessor(upstreamRes, requestId, requestModel);
    
    for await (const chunkStr of stream) {
      if (chunkStr.trim() === "data: [DONE]") break;
      if (!chunkStr.startsWith("data: ")) continue;
      
      try {
        const jsonStr = chunkStr.slice(6).trim();
        const chunk = JSON.parse(jsonStr);
        if (chunk.choices[0].delta.content) {
          fullContent += chunk.choices[0].delta.content;
        }
        if (chunk.choices[0].delta.reasoning_content) {
          fullReasoning += chunk.choices[0].delta.reasoning_content;
        }
      } catch (e) { /* ignore parse error in chunks */ }
    }

    // C·∫•u tr√∫c JSON tr·∫£ v·ªÅ chu·∫©n OpenAI Non-stream
    const responseBody = {
      id: requestId,
      object: "chat.completion",
      created: Math.floor(Date.now() / 1000),
      model: requestModel,
      choices: [{
        index: 0,
        message: {
          role: "assistant",
          content: fullContent,
          // M·ªôt s·ªë client h·ªó tr·ª£ field n√†y ·ªü non-stream (kh√¥ng chu·∫©n ho√†n to√†n nh∆∞ng h·ªØu √≠ch)
          reasoning_content: fullReasoning || undefined 
        },
        finish_reason: "stop"
      }],
      usage: { prompt_tokens: 0, completion_tokens: 0, total_tokens: 0 } // Dummy usage
    };

    return Response.json(responseBody, { headers: corsHeaders() });
  }
}

// --- [Server Entry] ---

console.log(`üöÄ Heck-2API (Bun) starting on port ${CONFIG.PORT}...`);

Bun.serve({
  port: CONFIG.PORT,
  async fetch(req) {
    const url = new URL(req.url);

    // CORS Preflight
    if (req.method === "OPTIONS") {
      return new Response(null, { status: 204, headers: corsHeaders() });
    }

    // Route: /v1/chat/completions
    if (url.pathname === "/v1/chat/completions" && req.method === "POST") {
      if (!verifyAuth(req)) return Response.json({ error: "Unauthorized" }, { status: 401 });
      return handleChatCompletions(req);
    }

    // Route: /v1/models
    if (url.pathname === "/v1/models" && req.method === "GET") {
      if (!verifyAuth(req)) return Response.json({ error: "Unauthorized" }, { status: 401 });
      const models = Object.keys(CONFIG.MODEL_MAP).map(id => ({
        id, object: "model", created: Date.now(), owned_by: "heck-bun"
      }));
      return Response.json({ object: "list", data: models }, { headers: corsHeaders() });
    }

    // Default: 404
    return Response.json({ error: "Not Found" }, { status: 404 });
  }
});
